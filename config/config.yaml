# data
reaction: True
data_name: slap_dataset_reactions.csv

# model
encoder:
  hidden_size: 300  # int
  depth: 3  # int, >= 3
  bias: False  # bool, whether to add bias to linear layers
  dropout_ratio: 0.0  # float, probability of dropout
  aggregation: mean  # str, options: [max, mean, sum]. Operation to aggregate node-centred feature vectors for graph
  activation: ReLU  # str, any activation from torch.nn can be used

decoder:
  hidden_size: 60  # int
  depth: 2  # int, number of hidden layers
  dropout_ratio: 0.0  # float, probability of dropout
  activation: ReLU  # str, any activation from torch.nn can be used
  hidden_bias: False  # bool, whether to add bias to hidden layers
  out_bias: False  # bool, whether to add bias to output layer
  out_sigmoid: False  # bool, whether to add a sigmoid layer at the end of the network

optimizer:
  weight_decay: 0.001
  lr: 0.0001
  lr_scheduler:
    epochs: 100
    lr_min: 1.0e-05
    lr_warmup_step: 5
    scheduler_name: exp_with_linear_warmup