# common settings
accelerator: cpu  # usually cpu or gpu.

# data
reaction: True
data_name: slap_dataset_reactions.csv
global_features: OHE  # str, options: {"RDKit", "FP", "OHE", None}. Global features to include with the graphs.
graph_type: bond_nodes  # str, options: {bond_edges, bond_nodes}. If bond_edges, build graph where atoms->nodes and
                        # bonds->edges. If bond_nodes, both atoms and bonds are encoded as edges.
featurizers: dgllife  # str, featurizers to use for atom and bond features. Options: {dgllife, chemprop}

# model
encoder:
  type: global_features  # str, options: {D-MPNN, GCN, global_features}. Note that global_features will ignore all other encoder configs.

decoder:
  type: LogisticRegression # str, options: {FFN, LogisticRegression, XGB}.

  # LogisticRegression parameters
  LogisticRegression:
    penalty: l2  # str, options: {l1, l2, elasticnet}. Type of regularization to use.
    solver: liblinear  # str, options: {lbfgs, liblinear, sag, saga, newton-cg}. Solver to use.
    C: 1.0  # float, inverse of the regularization strength Logistic Regression (i.e. smaller = stronger regularization)
    # l1_ratio: None  # float, ratio of L1 regularization to L2 regularization. Only used if penalty is elasticnet.

  # XGBoost parameters
  XGB:
    n_estimators: 100  # int, number of estimators for XGBoost
    learning_rate: 0.1  # float, learning rate for XGBoost
    max_depth: 6  # int, max depth for XGBoost
    gamma: 0.3  # float, for XGBoost:  Minimum loss reduction required to make a further partition on a leaf node of the tree.
    colsample_bytree: 0.5  # float, for XGBoost. Subsample ratio of columns when constructing each tree.
    reg_alpha: 0  # float, L1 regularization for XGBoost
    reg_lambda: 1  # float, L2 regularization for XGBoost

# FFN parameters
optimizer:
  weight_decay: 0
  lr: 0.0002
  lr_scheduler:
    epochs: 100  # should be the same as max_epochs for the desired effect
    lr_min: 0.00002
    lr_warmup_step: 2
    scheduler_name: exp_with_linear_warmup

training:
  max_epochs: 100
