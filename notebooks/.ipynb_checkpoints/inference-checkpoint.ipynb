{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c4535b",
   "metadata": {},
   "source": [
    "# Inference using the SLAP models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d90127",
   "metadata": {},
   "source": [
    "Predict reaction outcome for morpholine/piperazine products.\n",
    "\n",
    "For most requirements, using the command-line interface (`inference.py`) will be faster/simpler than using this notebook. But if you want to change things or have a look under the hood, feel free to use this notebook.\n",
    "\n",
    "The input to this are SMILES of the desired product(s).\n",
    "Inputs can be supplied directly as a csv file with one column named \"smiles\" and arbitrary additional columns.\n",
    "\n",
    "The output is:\n",
    "- all (one or two) reactionSMILES that lead to this product using the SLAP platform\n",
    "- for each reaction, a classification of whether the reaction is expected to work\n",
    "- for each reaction, a rough estimate of the confidence for this prediction\n",
    "\n",
    "The output is written to a new csv file containing all columns from the input file, and six new columns: `rxn1_smiles`, `rxn1_prediction`, `rxn1_confidence`, `rxn2_smiles`, `rxn2_prediction`, `rxn2_confidence`.\n",
    "\n",
    "Columns `rxn2_*` may have empty fields.\n",
    "\n",
    "Predictions are given as `0` (meaning no reaction expected) or `1` (meaning successful reaction expected). Only if the reaction is known, instead of the prediction, the mean of the known reaction outcome(s) is returned.\n",
    "Confidence is given as an integer in the range `0-4`, with `0` indicating the highest confidence.\n",
    "Confidence is determined based on the complexity of the prediction problem using the following heuristic:\n",
    "- `0`: known reactions\n",
    "- `1`: both reactants known in other reactions\n",
    "- `2`: exactly one reactant known in other reactions\n",
    "- `3`: unknown reactants, similar to training data\n",
    "- `4`: unknown reactants, dissimilar to training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d83b22f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No normalization for BCUT2D_MWHI\n",
      "WARNING:root:No normalization for BCUT2D_MWLOW\n",
      "WARNING:root:No normalization for BCUT2D_CHGHI\n",
      "WARNING:root:No normalization for BCUT2D_CHGLO\n",
      "WARNING:root:No normalization for BCUT2D_LOGPHI\n",
      "WARNING:root:No normalization for BCUT2D_LOGPLOW\n",
      "WARNING:root:No normalization for BCUT2D_MRHI\n",
      "WARNING:root:No normalization for BCUT2D_MRLOW\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import statistics\n",
    "import sys\n",
    "sys.path.append(str(pathlib.Path(\"__file__\").absolute().parents[1]))\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.model.classifier import load_trained_model\n",
    "from inference import import_valid_smiles_from_vl\n",
    "with warnings.catch_warnings(record=True):  \n",
    "    # ignore the descriptastorus package warning about missing normalizations\n",
    "    from src.data.dataloader import SLAPProductDataset, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d79eb05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    use_validation_data = True  # <-- toggle this to change the models used\n",
    "    \n",
    "    # paths to the best models\n",
    "    if use_validation_data:\n",
    "        # the next three are trained with full data, including validation plate data\n",
    "        model_0D = TRAINED_MODEL_DIR / \"2023-03-06-105610_484882\" / \"best.ckpt\"  # FFN\n",
    "        model_1D = TRAINED_MODEL_DIR / \"2023-03-06-112027_188465\" / \"best.ckpt\"  # D-MPNN\n",
    "        model_2D = TRAINED_MODEL_DIR / \"2023-03-06-112721_778803\" / \"best.ckpt\"  # D-MPNN\n",
    "        # path to the OneHotEncoder state for model_0D\n",
    "        ohe_state_dict = LOG_DIR / \"OHE_state_dict_bhTczANzKRRqIgUR.json\"  # with validation plate data\n",
    "        \n",
    "    else:\n",
    "        # the next three are trained without using validation plate data\n",
    "        model_0D = TRAINED_MODEL_DIR / \"2022-12-16-144509_863758\" / \"best.ckpt\"  # FFN\n",
    "        model_1D = TRAINED_MODEL_DIR / \"2022-12-16-145840_448790\" / \"best.ckpt\"  # D-MPNN\n",
    "        model_2D = TRAINED_MODEL_DIR / \"2022-12-06-115456_273019\" / \"best.ckpt\"  # D-MPNN\n",
    "        # path to the OneHotEncoder state for model_0D\n",
    "        ohe_state_dict = LOG_DIR / \"OHE_state_dict_FqIDTIsCHoURGQcv.json\"  # without validation plate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e1a3e0",
   "metadata": {},
   "source": [
    "To use the notebook on your products, change `raw_dir` to the directory that your CSV file containing SMILES is in. Then change `filename_base` to the filename of your csv file without the `.csv` suffix. If you do not want to use all the SMILES in your file (e.g. because some are not valid SLAP products), suppy a `valid_idx_file`. You can set the value to `None` if you want to use all SMILES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1ac6857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Could not generate reaction for product with index 2696. Using dummy reaction.\n",
      "Error leading to this warning: More than two possible reactions found.\n",
      "WARNING: Could not generate reaction for product with index 3457. Using dummy reaction.\n",
      "Error leading to this warning: Encountered RuntimeError while generating reaction for product 'CC(CC1COCC(C2=NO[C@@H]3CCCC[C@H]23)N1)C[Si](C)(C)C'.\n",
      "Original error message: More than one reaction found for SLAP reagent 'CC(CC(N)COC[Si](C)(C)C)C[Si](C)(C)C' and aldehyde 'O=CC1=NO[C@@H]2CCCC[C@H]12'.\n",
      "Reactions:\n",
      "C[Si](C)(C)[CH2:8][CH:7]([CH2:6][CH:4]([CH2:3][O:17][CH2:18][Si:19]([CH3:20])([CH3:21])[CH3:22])[NH2:5])[CH3:23].O=[CH:2][C:1]1=[N:9][O:11][C@H:13]2[C@@H:10]1[CH2:12][CH2:14][CH2:16][CH2:15]2>>[C:1]1([CH:2]2[NH:5][CH:4]([CH2:3][O:17][CH2:18][Si:19]([CH3:20])([CH3:21])[CH3:22])[CH2:6][CH:7]([CH3:23])[CH2:8]2)=[N:9][O:11][C@H:13]2[C@@H:10]1[CH2:12][CH2:14][CH2:16][CH2:15]2\n",
      "C[Si](C)(C)[CH2:8][O:7][CH2:6][CH:4]([CH2:3][CH:17]([CH3:18])[CH2:19][Si:20]([CH3:21])([CH3:22])[CH3:23])[NH2:5].O=[CH:2][C:1]1=[N:9][O:11][C@H:13]2[C@@H:10]1[CH2:12][CH2:14][CH2:16][CH2:15]2>>[C:1]1([CH:2]2[NH:5][CH:4]([CH2:3][CH:17]([CH3:18])[CH2:19][Si:20]([CH3:21])([CH3:22])[CH3:23])[CH2:6][O:7][CH2:8]2)=[N:9][O:11][C@H:13]2[C@@H:10]1[CH2:12][CH2:14][CH2:16][CH2:15]2\n",
      "WARNING: Could not generate reaction for product with index 3911. Using dummy reaction.\n",
      "Error leading to this warning: More than two possible reactions found.\n",
      "WARNING: Could not generate reaction for product with index 4542. Using dummy reaction.\n",
      "Error leading to this warning: More than two possible reactions found.\n",
      "WARNING: Could not generate reaction for product with index 5365. Using dummy reaction.\n",
      "Error leading to this warning: More than two possible reactions found.\n",
      "WARNING: Could not generate reaction for product with index 6368. Using dummy reaction.\n",
      "Error leading to this warning: Encountered RuntimeError while generating reaction for product 'CC1OCC(c2cc(S(N)(=O)=O)cs2)NC1C1CC[Si](C)(C)C1'.\n",
      "Original error message: More than one reaction found for SLAP reagent 'CC(OC[Si](C)(C)C)C(N)C1CC[Si](C)(C)C1' and aldehyde 'NS(=O)(=O)c1csc(C=O)c1'.\n",
      "Reactions:\n",
      "C[Si]1(C)[CH2:8][CH2:7][CH:6]([CH:4]([CH:3]([CH3:17])[O:18][CH2:19][Si:20]([CH3:21])([CH3:22])[CH3:23])[NH2:5])[CH2:24]1.O=[CH:2][c:1]1[cH:9][c:11]([S:13]([NH2:14])(=[O:15])=[O:16])[cH:12][s:10]1>>[c:1]1([CH:2]2[NH:5][CH:4]([CH:3]([CH3:17])[O:18][CH2:19][Si:20]([CH3:21])([CH3:22])[CH3:23])[CH:6]([CH3:24])[CH2:7][CH2:8]2)[cH:9][c:11]([S:13]([NH2:14])(=[O:15])=[O:16])[cH:12][s:10]1\n",
      "C[Si](C)(C)[CH2:8][O:7][CH:6]([CH:4]([CH:3]1[CH2:17][CH2:19][Si:20]([CH3:21])([CH3:22])[CH2:18]1)[NH2:5])[CH3:23].O=[CH:2][c:1]1[cH:9][c:11]([S:13]([NH2:14])(=[O:15])=[O:16])[cH:12][s:10]1>>[c:1]1([CH:2]2[NH:5][CH:4]([CH:3]3[CH2:17][CH2:19][Si:20]([CH3:21])([CH3:22])[CH2:18]3)[CH:6]([CH3:23])[O:7][CH2:8]2)[cH:9][c:11]([S:13]([NH2:14])(=[O:15])=[O:16])[cH:12][s:10]1\n",
      "WARNING: Could not generate reaction for product with index 7596. Using dummy reaction.\n",
      "Error leading to this warning: Encountered RuntimeError while generating reaction for product 'CC1OCC(c2c(O)ccc(Cl)c2Cl)NC1C1CC[Si](C)(C)C1'.\n",
      "Original error message: More than one reaction found for SLAP reagent 'CC(OC[Si](C)(C)C)C(N)C1CC[Si](C)(C)C1' and aldehyde 'O=Cc1c(O)ccc(Cl)c1Cl'.\n",
      "Reactions:\n",
      "C[Si]1(C)[CH2:8][CH2:7][CH:6]([CH:4]([CH:3]([CH3:17])[O:18][CH2:19][Si:20]([CH3:21])([CH3:22])[CH3:23])[NH2:5])[CH2:24]1.O=[CH:2][c:1]1[c:9]([OH:11])[cH:12][cH:15][c:13]([Cl:16])[c:10]1[Cl:14]>>[c:1]1([CH:2]2[NH:5][CH:4]([CH:3]([CH3:17])[O:18][CH2:19][Si:20]([CH3:21])([CH3:22])[CH3:23])[CH:6]([CH3:24])[CH2:7][CH2:8]2)[c:9]([OH:11])[cH:12][cH:15][c:13]([Cl:16])[c:10]1[Cl:14]\n",
      "C[Si](C)(C)[CH2:8][O:7][CH:6]([CH:4]([CH:3]1[CH2:17][CH2:19][Si:20]([CH3:21])([CH3:22])[CH2:18]1)[NH2:5])[CH3:23].O=[CH:2][c:1]1[c:9]([OH:11])[cH:12][cH:15][c:13]([Cl:16])[c:10]1[Cl:14]>>[c:1]1([CH:2]2[NH:5][CH:4]([CH:3]3[CH2:17][CH2:19][Si:20]([CH3:21])([CH3:22])[CH2:18]3)[CH:6]([CH3:23])[O:7][CH2:8]2)[c:9]([OH:11])[cH:12][cH:15][c:13]([Cl:16])[c:10]1[Cl:14]\n",
      "WARNING: Could not generate reaction for product with index 7690. Using dummy reaction.\n",
      "Error leading to this warning: Encountered RuntimeError while generating reaction for product 'CCOC(=O)N1CC(CC2CCOC2)NC(C2CC[Si](C)(C)C2)C1'.\n",
      "Original error message: More than one reaction found for SLAP reagent 'CCOC(=O)N(CC(N)C1CC[Si](C)(C)C1)C[Si](C)(C)C' and aldehyde 'O=CCC1CCOC1'.\n",
      "Reactions:\n",
      "C[Si]1(C)[CH2:8][CH2:7][CH:6]([CH:4]([CH2:3][N:14]([C:15]([O:17][CH2:20][CH3:24])=[O:18])[CH2:16][Si:19]([CH3:21])([CH3:22])[CH3:23])[NH2:5])[CH2:25]1.O=[CH:2][CH2:1][CH:9]1[CH2:10][CH2:12][O:13][CH2:11]1>>[CH2:1]([CH:2]1[NH:5][CH:4]([CH2:3][N:14]([C:15]([O:17][CH2:20][CH3:24])=[O:18])[CH2:16][Si:19]([CH3:21])([CH3:22])[CH3:23])[CH:6]([CH3:25])[CH2:7][CH2:8]1)[CH:9]1[CH2:10][CH2:12][O:13][CH2:11]1\n",
      "C[Si](C)(C)[CH2:8][N:7]([CH2:6][CH:4]([CH:3]1[CH2:14][CH2:16][Si:17]([CH3:18])([CH3:19])[CH2:15]1)[NH2:5])[C:20]([O:21][CH2:23][CH3:24])=[O:22].O=[CH:2][CH2:1][CH:9]1[CH2:10][CH2:12][O:13][CH2:11]1>>[CH2:1]([CH:2]1[NH:5][CH:4]([CH:3]2[CH2:14][CH2:16][Si:17]([CH3:18])([CH3:19])[CH2:15]2)[CH2:6][N:7]([C:20]([O:21][CH2:23][CH3:24])=[O:22])[CH2:8]1)[CH:9]1[CH2:10][CH2:12][O:13][CH2:11]1\n",
      "WARNING: Could not generate reaction for product with index 8156. Using dummy reaction.\n",
      "Error leading to this warning: More than two possible reactions found.\n",
      "WARNING: Could not generate reaction for product with index 8729. Using dummy reaction.\n",
      "Error leading to this warning: Encountered RuntimeError while generating reaction for product 'C#CCOc1ccccc1C1COCC(CC(C)C[Si](C)(C)C)N1'.\n",
      "Original error message: More than one reaction found for SLAP reagent 'CC(CC(N)COC[Si](C)(C)C)C[Si](C)(C)C' and aldehyde 'C#CCOc1ccccc1C=O'.\n",
      "Reactions:\n",
      "C[Si](C)(C)[CH2:8][CH:7]([CH2:6][CH:4]([CH2:3][O:18][CH2:19][Si:20]([CH3:21])([CH3:22])[CH3:23])[NH2:5])[CH3:24].O=[CH:2][c:1]1[cH:9][cH:11][cH:14][cH:13][c:10]1[O:12][CH2:15][C:16]#[CH:17]>>[c:1]1([CH:2]2[NH:5][CH:4]([CH2:3][O:18][CH2:19][Si:20]([CH3:21])([CH3:22])[CH3:23])[CH2:6][CH:7]([CH3:24])[CH2:8]2)[cH:9][cH:11][cH:14][cH:13][c:10]1[O:12][CH2:15][C:16]#[CH:17]\n",
      "C[Si](C)(C)[CH2:8][O:7][CH2:6][CH:4]([CH2:3][CH:18]([CH3:19])[CH2:20][Si:21]([CH3:22])([CH3:23])[CH3:24])[NH2:5].O=[CH:2][c:1]1[cH:9][cH:11][cH:14][cH:13][c:10]1[O:12][CH2:15][C:16]#[CH:17]>>[c:1]1([CH:2]2[NH:5][CH:4]([CH2:3][CH:18]([CH3:19])[CH2:20][Si:21]([CH3:22])([CH3:23])[CH3:24])[CH2:6][O:7][CH2:8]2)[cH:9][cH:11][cH:14][cH:13][c:10]1[O:12][CH2:15][C:16]#[CH:17]\n",
      "INFO: 8733 SMILES were read. For 10 SMILES, no valid SLAP reaction could be generated.\n"
     ]
    }
   ],
   "source": [
    "# Import product SMILES and generate reactionSMILES. This will take some time.\n",
    "# This will throw warnings if any reactions cannot be generated, \n",
    "# e.g. if there are two morpholines in the same product.\n",
    "raw_dir = pathlib.Path(\"../data/VL\")  # <-- change me\n",
    "filename_base = \"VL_smiles_chunk_00961\"  # <-- change me\n",
    "valid_idx_file = raw_dir / f\"{filename_base}_valid.csv\"  # <-- change me or set me to None\n",
    "df = import_valid_smiles_from_vl(raw_dir, filename_base, valid_idx_file=valid_idx_file)\n",
    "data = SLAPProductDataset(smiles=df[\"smiles\"].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "069d3043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data. This includes generating reaction graphs and takes some time.\n",
    "\n",
    "data.process({\"dataset_0D\": dict(\n",
    "    reaction=True, \n",
    "    global_features=[\"OHE\",],\n",
    "    global_featurizer_state_dict_path=ohe_state_dict,\n",
    "    graph_type=\"bond_edges\", \n",
    "    featurizers=\"custom\",\n",
    "),\n",
    "             \"dataset_1D_slap\": dict(\n",
    "    reaction=True, \n",
    "    global_features=None, \n",
    "    graph_type=\"bond_nodes\", \n",
    "    featurizers=\"custom\",\n",
    "),\n",
    "              \"dataset_1D_aldehyde\": dict(\n",
    "    reaction=True, \n",
    "    global_features=None, \n",
    "    graph_type=\"bond_nodes\", \n",
    "    featurizers=\"custom\",\n",
    "),\n",
    "              \"dataset_2D\": dict(\n",
    "    reaction=True, \n",
    "    global_features=None, \n",
    "    graph_type=\"bond_nodes\", \n",
    "    featurizers=\"custom\",\n",
    "),\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dbf0d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/miniconda3/envs/slap-gnn/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/julian/miniconda3/envs/slap-gnn/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010430097579956055,
       "initial": 0,
       "n": 0,
       "ncols": 100,
       "nrows": 32,
       "postfix": null,
       "prefix": "Predicting",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23035dce27148edb187fc68437be1c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/PycharmProjects/slap-gnn/src/data/dataloader.py:41: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1656352464346/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  batched_global_features = torch.tensor(global_features, dtype=torch.float32)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008572578430175781,
       "initial": 0,
       "n": 0,
       "ncols": 100,
       "nrows": 32,
       "postfix": null,
       "prefix": "Predicting",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea48169628bc485aaf62ef88ab2edd1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009615421295166016,
       "initial": 0,
       "n": 0,
       "ncols": 100,
       "nrows": 32,
       "postfix": null,
       "prefix": "Predicting",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4953f2e0e74e45a1b521185a1ef2ff6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015076398849487305,
       "initial": 0,
       "n": 0,
       "ncols": 100,
       "nrows": 32,
       "postfix": null,
       "prefix": "Predicting",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db2e3593ee04d57b897d2cb3f3d9c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run all the predictions\n",
    "\n",
    "if data.dataset_0D:\n",
    "    # load the trained model if it is not loaded\n",
    "    if isinstance(model_0D, str):\n",
    "        model_0D = load_trained_model(\"FFN\", model_0D)\n",
    "        model_0D.eval()\n",
    "    trainer = pl.Trainer(accelerator=\"gpu\", logger=False, max_epochs=-1)\n",
    "    dl = DataLoader(data.dataset_0D, collate_fn=collate_fn)\n",
    "    probabilities_0D = torch.concat(trainer.predict(model_0D, dl))\n",
    "    predictions_0D = (probabilities_0D > 0.5).numpy().astype(float)\n",
    "    \n",
    "\n",
    "if data.dataset_1D_aldehyde:\n",
    "    # load the trained model if it is not loaded\n",
    "    if isinstance(model_1D, str):\n",
    "        model_1D = load_trained_model(\"D-MPNN\", model_1D)\n",
    "        model_1D.eval()\n",
    "    trainer = pl.Trainer(accelerator=\"gpu\", logger=False, max_epochs=-1)\n",
    "    dl = DataLoader(data.dataset_1D_aldehyde, collate_fn=collate_fn)\n",
    "    probabilities_1D_aldehyde = torch.concat(trainer.predict(model_1D, dl))\n",
    "    predictions_1D_aldehyde = (probabilities_1D_aldehyde > 0.5).numpy().astype(float)\n",
    "\n",
    "if data.dataset_1D_slap:\n",
    "    # load the trained model if it is not loaded\n",
    "    if isinstance(model_1D, str):\n",
    "        model_1D = load_trained_model(\"D-MPNN\", model_1D)\n",
    "        model_1D.eval()\n",
    "    trainer = pl.Trainer(accelerator=\"gpu\", logger=False, max_epochs=-1)\n",
    "    dl = DataLoader(data.dataset_1D_slap, collate_fn=collate_fn)\n",
    "    probabilities_1D_slap = torch.concat(trainer.predict(model_1D, dl))\n",
    "    predictions_1D_slap = (probabilities_1D_slap > 0.5).numpy().astype(float)\n",
    "\n",
    "if data.dataset_2D:\n",
    "    # load the trained model if it is not loaded\n",
    "    if isinstance(model_2D, str):\n",
    "        model_2D = load_trained_model(\"D-MPNN\", model_2D)\n",
    "        model_2D.eval()\n",
    "    trainer = pl.Trainer(accelerator=\"gpu\", logger=False, max_epochs=-1)\n",
    "    dl = DataLoader(data.dataset_2D, collate_fn=collate_fn)\n",
    "    probabilities_2D = torch.concat(trainer.predict(model_2D, dl))\n",
    "    predictions_2D = (probabilities_2D > 0.5).numpy().astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89faead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble outputs\n",
    "predictions = np.full(len(data.reactions), np.nan, dtype=float)\n",
    "\n",
    "predictions[data.idx_known] = [statistics.mean(data.known_outcomes[i]) for i in data.idx_known]  # for known reaction we add the average reaction outcome\n",
    "try:\n",
    "    predictions[data.idx_0D] = predictions_0D\n",
    "except NameError:\n",
    "    pass\n",
    "try:\n",
    "    predictions[data.idx_1D_slap] = predictions_1D_slap\n",
    "except NameError:\n",
    "    pass\n",
    "try:\n",
    "    predictions[data.idx_1D_aldehyde] = predictions_1D_aldehyde\n",
    "except NameError:\n",
    "    pass\n",
    "try:\n",
    "    predictions[data.idx_2D] = predictions_2D\n",
    "except NameError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13c63465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we have not predicted for anything\n",
    "# this should be only the reactions in data.invalid_idxs\n",
    "rxn_idxs_no_pred = np.argwhere(np.isnan(predictions)).flatten()\n",
    "\n",
    "rxn_idxs_invalid = [data.product_idxs.index(i) for i in data.invalid_idxs]\n",
    "\n",
    "assert set(rxn_idxs_no_pred) == set(rxn_idxs_invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33ea6070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the 1D- product_idxs to the directionally reverse 2D indices\n",
    "arr = np.full((len(data.smiles), 2), fill_value=-1)\n",
    "last_idx = -1\n",
    "for i, idx in enumerate(data.product_idxs):\n",
    "    if idx == last_idx:\n",
    "        arr[idx, 1] = i\n",
    "    else:\n",
    "        last_idx = idx\n",
    "        arr[idx, 0] = i\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d67ad65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_dict = {\n",
    "    \"known\": 0,\n",
    "    \"0D\": 1,\n",
    "    \"1D_SLAP\": 2,\n",
    "    \"1D_aldehyde\": 2,\n",
    "    \"2D_similar\": 3,\n",
    "    \"2D_dissimilar\": 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4bd3b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate problem type to integer\n",
    "rxn_problem_types = list(map(confidence_dict.get, data.problem_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fbeea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add a nonsense value to the end of each of these lists so that indexing with -1 will return the nonsense value\n",
    "reactions_augmented = data.reactions + [\"\"]\n",
    "predictions_augmented = list(predictions) + [np.nan]\n",
    "rxn_problem_types_augmented = rxn_problem_types + [99]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c3c78e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain individual new columns for output df\n",
    "df[\"rxn1_smiles\"] = [data.reactions[i] for i in arr[:,0]]\n",
    "\n",
    "df[\"rxn1_predictions\"] = [predictions[i] for i in arr[:,0]]\n",
    "\n",
    "df[\"rxn1_confidence\"] = [rxn_problem_types[i] for i in arr[:,0]]\n",
    "\n",
    "df[\"rxn2_smiles\"] = [reactions_augmented[i] for i in arr[:,1]]\n",
    "\n",
    "df[\"rxn2_predictions\"] = [predictions_augmented[i] for i in arr[:,1]]\n",
    "\n",
    "df[\"rxn2_confidence\"] = [rxn_problem_types_augmented[i] for i in arr[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59bff6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13660 reactions generated from 8733 input SMILES\n",
      "Known reactions: 0\n",
      "0D reactions: 0, thereof 0 predicted positive\n",
      "1D reactions with unknown aldehyde: 52, thereof 29 predicted positive\n",
      "1D reactions with unknown SLAP reagent: 245, thereof 104 predicted positive\n",
      "2D reactions: 13352, thereof 4756 predicted positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write dataset statistics for control to log file (+ optionally print)\n",
    "verbose = True\n",
    "log_output = f\"\"\"\\\n",
    "{len(data.reactions)} reactions generated from {len(data.smiles)} input SMILES\n",
    "Known reactions: {(sum(x is not None for x in data.known_outcomes))}\n",
    "0D reactions: 0, thereof 0 predicted positive\n",
    "1D reactions with unknown aldehyde: {len(data.dataset_1D_aldehyde)}, thereof {np.count_nonzero(predictions_1D_aldehyde)} predicted positive\n",
    "1D reactions with unknown SLAP reagent: {len(data.dataset_1D_slap)}, thereof {np.count_nonzero(predictions_1D_slap)} predicted positive\n",
    "2D reactions: {len(data.dataset_2D)}, thereof {np.count_nonzero(predictions_2D)} predicted positive\n",
    "\"\"\"\n",
    "\n",
    "with open(raw_dir / f\"{filename_base}_reaction_prediction.log\", \"w\") as file:\n",
    "    file.write(log_output)\n",
    "if verbose:\n",
    "    print(log_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec60f281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write df to output file\n",
    "df.to_csv(raw_dir / f\"{filename_base}_reaction_prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b7e5d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
